{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnanthSankaralingam/kv-cache-research/blob/main/HyperAttention_for_Longbench.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## imports and setup"
      ],
      "metadata": {
        "id": "gYS76wODRzzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install necessary packages\n",
        "!pip uninstall transformers -y  # uninstall any existing transformers package\n",
        "!pip install triton==2.0.0 --no-deps # TAKEN FROM README, not sure why .dev version doesnt exist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLjGlFkOSpu1",
        "outputId": "e2e86ffd-10a5-43c0-d642-361253e1faa9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "Successfully installed triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers einops -q  # install the latest versions of transformers, einops, and triton"
      ],
      "metadata": {
        "id": "_E5oJm34TPGa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the hyper-attention repository to access additional required files\n",
        "!git clone https://github.com/insuhan/hyper-attn.git\n",
        "\n",
        "# change directory to the cloned repository\n",
        "%cd hyper-attn\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg5XUn-jS3wX",
        "outputId": "b6aef302-cf78-46a1-d9b3-7a7b167adda6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hyper-attn'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 33 (delta 10), reused 7 (delta 7), pack-reused 12 (from 1)\u001b[K\n",
            "Receiving objects: 100% (33/33), 108.29 KiB | 21.66 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n",
            "/content/hyper-attn/hyper-attn\n",
            "assets\tbenchmark_patch_llm.py\tbenchmark_single_attention.py  LICENSE\tmodels\tREADME.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm  # progress bar for loops\n",
        "from torch.nn import CrossEntropyLoss  # loss function for classification tasks\n",
        "\n",
        "import transformers\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "\n",
        "# import model and tokenizer classes from transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers.models.llama.modeling_llama import LlamaAttention, LlamaRotaryEmbedding\n",
        "\n",
        "os.environ['HUGGING_FACE_HUB_TOKEN'] = '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'\n",
        "\n",
        "# add the hyper-attn directory to the python path\n",
        "sys.path.append('/content/hyper-attn')\n",
        "sys.path.append('/content/hyper-attn/models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC4ZJULmRzXI",
        "outputId": "4d62eca1-931c-4231-faa2-90f2b10fe781"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers version: 4.46.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POC testing hyper attention through direct class declaration"
      ],
      "metadata": {
        "id": "j6pFsrsjWO63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from models.attention.hyper_attn import HyperAttention\n",
        "\n",
        "attn = HyperAttention(\n",
        "    input_dim=64,\n",
        "    lsh_num_projs=7,\n",
        "    block_size=256,\n",
        "    sample_size=256,\n",
        "    min_seq_len=4096)\n",
        "\n",
        "# dummy dimensions\n",
        "batch_size = 2\n",
        "seq_len = 4096\n",
        "input_dim = 64\n",
        "n_heads = 8 # num attention heads\n",
        "\n",
        "# dummy tensors for q k v\n",
        "query = torch.rand(batch_size, n_heads, seq_len, input_dim)\n",
        "key = torch.rand(batch_size, n_heads, seq_len, input_dim)\n",
        "value = torch.rand(batch_size, n_heads, seq_len, input_dim)\n",
        "\n",
        "# Forward pass\n",
        "attn_output = attn(query, key, value, True)\n",
        "\n",
        "print(\"Query shape:\", query.shape)\n",
        "print(\"Key shape:\", key.shape)\n",
        "print(\"Value shape:\", value.shape)\n",
        "print(\"Attention output shape:\", attn_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuHDQas8UH6r",
        "outputId": "c4d5d5b6-44fb-4fd5-f17c-a443105cbc3d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query shape: torch.Size([2, 8, 4096, 64])\n",
            "Key shape: torch.Size([2, 8, 4096, 64])\n",
            "Value shape: torch.Size([2, 8, 4096, 64])\n",
            "Attention output shape: torch.Size([2, 8, 4096, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update hyperattn to not use flash attn"
      ],
      "metadata": {
        "id": "j_n8BbQfdLJH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kATRL3Fh0SY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def exact_attention(q, k, v, softmax_scale, causal=False, bias=None):\n",
        "    # compute the attention scores\n",
        "    qk = torch.matmul(q, k.transpose(-2, -1)) * softmax_scale\n",
        "    if bias is not None:\n",
        "        qk = qk + bias\n",
        "    if causal:\n",
        "        # apply causal mask to prevent attention to future tokens\n",
        "        mask = torch.tril(torch.ones(qk.shape[-2], qk.shape[-1], device=qk.device)).unsqueeze(0).unsqueeze(0)\n",
        "        qk = qk.masked_fill(mask == 0, float('-inf'))\n",
        "    # apply softmax to get attention probabilities\n",
        "    qk = torch.softmax(qk, dim=-1)\n",
        "    # compute the output by weighting the values with attention probabilities\n",
        "    output = torch.matmul(qk, v)\n",
        "    return output, None\n",
        "\n",
        "# define utility functions for rotary embeddings\n",
        "def rotate_half(x):\n",
        "    # split the last dimension into two halves and concatenate them with flipped signs\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "    # apply rotary positional embeddings to query and key tensors\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "    # define the custom attention class using HyperAttention\n",
        "class LlamaHyperAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = config.hidden_size  # total hidden size\n",
        "        self.num_heads = config.num_attention_heads  # number of attention heads\n",
        "        self.head_dim = self.hidden_size // self.num_heads  # dimension per head\n",
        "\n",
        "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
        "            raise ValueError(\n",
        "                f\"hidden_size must be divisible by num_attention_heads (got `hidden_size`: {self.hidden_size} and \"\n",
        "                f\"`num_attention_heads`: {self.num_heads}).\"\n",
        "            )\n",
        "\n",
        "        # linear projections for query, key, value, and output\n",
        "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "\n",
        "        # rotary positional embeddings\n",
        "        self.rotary_emb = LlamaRotaryEmbedding(\n",
        "            self.head_dim,\n",
        "            max_position_embeddings=config.max_position_embeddings,\n",
        "            base=config.rope_theta,\n",
        "            rope_scaling=config.rope_scaling,\n",
        "        )\n",
        "\n",
        "        # initialize hyper attention\n",
        "        self.hyper_attn = HyperAttention(\n",
        "            input_dim=self.head_dim,\n",
        "            lsh_num_projs=getattr(config, 'lsh_num_projs', 7),\n",
        "            block_size=getattr(config, 'block_size', 256),\n",
        "            sample_size=getattr(config, 'sample_size', 256),\n",
        "            min_seq_len=getattr(config, 'min_seq_len', 4096),\n",
        "            cuda=False,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        position_ids=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "        use_cache=False,\n",
        "        **kwargs\n",
        "    ):\n",
        "        bsz, seq_len, _ = hidden_states.size()\n",
        "        device = hidden_states.device\n",
        "\n",
        "        # ensure consistent data types\n",
        "        hidden_states = hidden_states.to(dtype=self.q_proj.weight.dtype)\n",
        "\n",
        "        # project hidden_states to query, key, and value tensors\n",
        "        query_states = self.q_proj(hidden_states)\n",
        "        key_states = self.k_proj(hidden_states)\n",
        "        value_states = self.v_proj(hidden_states)\n",
        "\n",
        "        # reshape and split into heads\n",
        "        query_states = query_states.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key_states = key_states.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        value_states = value_states.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # apply rotary embeddings\n",
        "        cos, sin = self.rotary_emb(query_states, seq_len=seq_len)\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
        "\n",
        "        # use hyper attention\n",
        "        attn_output, _ = self.hyper_attn(\n",
        "            query_states,\n",
        "            key_states,\n",
        "            value_states,\n",
        "            scale=None,\n",
        "            causal=True,\n",
        "            return_lse=False\n",
        "        )\n",
        "\n",
        "        # merge heads\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, seq_len, self.hidden_size)\n",
        "\n",
        "        # project output\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "\n",
        "        if output_attentions:\n",
        "            # hyper attention does not return attention weights\n",
        "            attn_weights = None\n",
        "            outputs = (attn_output, attn_weights)\n",
        "        else:\n",
        "            outputs = (attn_output,)\n",
        "\n",
        "        if use_cache:\n",
        "            # return key and value states for caching\n",
        "            present_key_value = (key_states, value_states)\n",
        "            outputs = outputs + (present_key_value,)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replace attention modules in llama 3"
      ],
      "metadata": {
        "id": "BQ7_E1KQnB7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to replace modules in the model\n",
        "def replace_module(model, module_name, new_module):\n",
        "    # replace a module in the model with a new module\n",
        "    components = module_name.split('.')\n",
        "    attr = components[-1]\n",
        "    parent = model\n",
        "    for comp in components[:-1]:\n",
        "        parent = getattr(parent, comp)\n",
        "    setattr(parent, attr, new_module)\n",
        "\n",
        "# function to patch attention layers in the model\n",
        "def patch_attention_layers(model, **kwargs):\n",
        "    # replace LlamaAttention modules with LlamaHyperAttention modules\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, LlamaAttention):\n",
        "            # create a new LlamaHyperAttention module\n",
        "            new_module = LlamaHyperAttention(model.config)\n",
        "            # copy weights from the original attention module\n",
        "            new_module.q_proj.weight.data = module.q_proj.weight.data\n",
        "            new_module.k_proj.weight.data = module.k_proj.weight.data\n",
        "            new_module.v_proj.weight.data = module.v_proj.weight.data\n",
        "            new_module.o_proj.weight.data = module.o_proj.weight.data\n",
        "            # replace the attention module in the model\n",
        "            replace_module(model, name, new_module)\n",
        "\n",
        "# function to get the model and tokenizer\n",
        "def get_model_and_tokenizer(model_name):\n",
        "    if model_name == \"llama-3.1-8b-instruct\":\n",
        "        # load the tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "            use_auth_token=os.environ['HUGGING_FACE_HUB_TOKEN'],\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        # load the model\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map='auto',\n",
        "            use_auth_token=os.environ['HUGGING_FACE_HUB_TOKEN'],\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError(\"Currently we only support llama-3.1-8b-instruct\")\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "U2TuYRLdnE7L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # define arguments\n",
        "    class Args:\n",
        "        seq_len = 1024  # adjust based on your GPU memory\n",
        "        patch_config = 'last'\n",
        "        attn_method = 'hyper'\n",
        "        num_patch_layers = -1\n",
        "        block_size = 256\n",
        "        sample_size = 256\n",
        "        lsh_num_projs = 7\n",
        "        min_seq_len = 4096\n",
        "        model_name = 'llama-3.1-8b-instruct'\n",
        "\n",
        "    args = Args()\n",
        "    for arg_name, arg_var in vars(args).items():\n",
        "        print(f\"{arg_name:<16} : {arg_var}\")\n",
        "\n",
        "    # load the model and tokenizer\n",
        "    model, tokenizer = get_model_and_tokenizer(args.model_name)\n",
        "    tokenizer.model_max_length = args.seq_len\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dtype = torch.float16  # use float16 to save memory\n",
        "\n",
        "    # prepare dummy data for testing (replace with your dataset if needed)\n",
        "    data = [{\"context\": \"This is a test sentence. \" * (args.seq_len // 5)} for _ in range(1)]  # adjust sequence length\n",
        "    encoded_texts = []\n",
        "\n",
        "    for data_i in data:\n",
        "        encoded_text = tokenizer(\n",
        "            data_i['context'],\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            max_length=args.seq_len\n",
        "        )['input_ids']\n",
        "        if encoded_text.size(1) < args.seq_len:\n",
        "            continue\n",
        "        encoded_texts.append(encoded_text)\n",
        "\n",
        "    print(f\"# of data longer than {args.seq_len}: {len(encoded_texts)}\")\n",
        "\n",
        "    # patch the attention layers with LlamaHyperAttention\n",
        "    if args.attn_method != 'flash':\n",
        "        patch_attention_layers(model=model, **vars(args))\n",
        "\n",
        "    model.eval()\n",
        "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
        "\n",
        "    ppls = []\n",
        "\n",
        "    pbar = tqdm(range(len(encoded_texts)))\n",
        "    for bid in pbar:\n",
        "        encoded_batch = encoded_texts[bid].to(device)\n",
        "        attn_mask = torch.ones_like(encoded_batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=encoded_batch, attention_mask=attn_mask, use_cache=False)\n",
        "            out_logits = outputs.logits\n",
        "\n",
        "        labels = encoded_batch\n",
        "\n",
        "        shift_logits = out_logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "        shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n",
        "\n",
        "        loss_ = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)).float()\n",
        "        loss_ = loss_.view(shift_labels.size())\n",
        "        perplexity_batch = torch.exp2(\n",
        "            (loss_ * shift_attention_mask_batch).sum(1)\n",
        "            / shift_attention_mask_batch.sum(1)\n",
        "        )\n",
        "        ppls += perplexity_batch.tolist()\n",
        "\n",
        "        avg_ppl = np.mean([p for p in ppls if not np.isnan(p)])\n",
        "        pbar.set_description(f\"[{bid + 1}/{len(encoded_texts)}] avg_ppl: {avg_ppl:.4f}\")\n",
        "\n",
        "        # clear variables to free memory\n",
        "        del outputs, out_logits, encoded_batch, attn_mask, shift_logits, shift_labels, shift_attention_mask_batch, loss_, perplexity_batch\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    nan_cnt = sum(np.isnan(np.array(ppls)))\n",
        "    ppl_mean = np.mean(np.array(ppls)[~np.isnan(np.array(ppls))])\n",
        "\n",
        "    print(f\"Perplexity: {ppl_mean}, NaN count: {nan_cnt}\")\n",
        "    res_str = f\"Model: {args.model_name}, dtype: {dtype}, seq_len: {args.seq_len}, num_patch_layers: {args.num_patch_layers}, n_data: {len(encoded_texts)}, Perplexity: {ppl_mean}, NaN count: {nan_cnt}\\n\"\n",
        "    print(res_str)\n",
        "\n",
        "# run the main function\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "-KhY3JS8nO93",
        "outputId": "72f2297c-42a6-4999-f797-9fc63626d2e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_model_and_tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f241e8f8f834>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# run the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-f241e8f8f834>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# load the model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_and_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_max_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_model_and_tokenizer' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}